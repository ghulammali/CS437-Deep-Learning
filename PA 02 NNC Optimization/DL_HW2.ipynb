{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TUCOENwWr1fa"
   },
   "source": [
    "# CS 437 - Deep Learning - Assignment 2\n",
    "\n",
    "*__Submission Instructions:__*\n",
    "- Rename this notebook to `hw2_rollnumber.ipynb` before submission on LMS.\n",
    "- All code must be written in this notebook (you do not need to submit any other files).\n",
    "- The output of all cells must be present in the version of the notebook you submit.\n",
    "- The university honor code should be maintained. Any violation, if found, will result in disciplinary action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T11:20:45.525290Z",
     "start_time": "2019-02-12T11:20:45.502549Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OnYpH2Phr1fc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import pydot\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "import keras.backend as k\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from IPython.display import Image\n",
    "from ipywidgets import interact, fixed, IntSlider, Play, HBox, jslink\n",
    "\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TAW8-_-Qr1fh"
   },
   "source": [
    "**Please write your roll number in the next cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T11:12:20.463527Z",
     "start_time": "2019-02-12T11:12:20.453032Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "CZaGL-v3r1fi"
   },
   "outputs": [],
   "source": [
    "rollnumber = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGMltRrBr1fm"
   },
   "source": [
    "In this assignment you will be exploring the model parameter space and how the different initializations affect the error surface and the optimization process as a whole. \n",
    "\n",
    "Unlike Assignment 1, the tasks this time are disjointed, and can be attempted in any order. \n",
    "\n",
    "Take a close look at the `__init__` and `fit` methods for all parts to understand what parameters are available to you and what instance variables have been (if any) setup for you to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T11:12:21.066342Z",
     "start_time": "2019-02-12T11:12:21.041867Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sUvsTNgTr1fn"
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "# If you don't fully understand these functions don't worry\n",
    "# You aren't supposed to understand this per se\n",
    "\n",
    "def get_error_surface(w1, w2, cls, data):\n",
    "    model = NeuralNetwork('sgd')\n",
    "    z = np.zeros_like(w1)\n",
    "    for row in range(w1.shape[0]):\n",
    "        for col in range(w1.shape[1]):\n",
    "            model.weights_[0] = np.array([w1[row,col], w2[row,col]])\n",
    "            loss = model.evaluate(*data)\n",
    "            z[row,col] = loss\n",
    "    return z\n",
    "\n",
    "def plot_decision_boundary(pred_func, x_min, x_max, y_min, y_max, cmap, ax):\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole ggid\n",
    "    Z = pred_func(np.c_[xx.flatten(), yy.flatten()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour\n",
    "    ax.contourf(xx, yy, Z, cmap=cmap, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_PBhC_Vyr1fq"
   },
   "source": [
    "## Task 1 - Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qe89t24br1fr"
   },
   "source": [
    "In this task you will start with the `NeuralNetwork` class from Assignment 1 (task5) which used Gradient Descent, and modify it to allow for different optimizers. The ones which are compulsory to implement are\n",
    "- Gradient Descent (already done)\n",
    "- Momentum\n",
    "\n",
    "Implement any two of the following optimizers\n",
    "- Nesterov Accelerated Gradient\n",
    "- RMS Prop\n",
    "- Adam\n",
    "- Adagrad\n",
    "\n",
    "You will only be modifying the `weight_update` method. Take a close look at the `__init__` method to see which instance variables are available to you (if you want, you can also make your own). You can modify the `fit` function to take as input any hyperparameters ($\\gamma$, $\\eta$ etc.) required to be passed onto the `weight_update` method. \n",
    "\n",
    "You have been provided with visualization code to show the error surface and the path the model takes during training. You should be able to see how the error surface changes for different datasets and how the optimization path changes due to a different optimizer.\n",
    "\n",
    "*__Note:__ For ease of plotting the model is restricted to just having 2 parameters. This means that the model will be a perceptron with 2 inputs (and no bias )so it can learn a regression function of two inputs. Picture of the model shown here.*\n",
    "![](./perceptron.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T10:53:40.674435Z",
     "start_time": "2019-02-12T10:53:40.631229Z"
    },
    "code_folding": [
     0,
     23,
     28,
     49,
     61,
     93,
     97
    ],
    "colab": {},
    "colab_type": "code",
    "id": "HP-dpONar1ft"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    @staticmethod\n",
    "    def mean_squared_error(y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def __init__(self, optimizer):\n",
    "        '''Creates a Feed-Forward Neural Network.\n",
    "        \"optimizer\" is the particular gradient descent strategy used for weight update'''\n",
    "        self.num_layers = 2 # includes input layer\n",
    "        self.nodes_per_layer = [2,1]\n",
    "        self.input_shape = 2\n",
    "        self.output_shape = 1\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Values needed for more sophisticated optimizers which require saved state\n",
    "        self.past_update_amount = 0\n",
    "        \n",
    "        self.__init_weights()\n",
    "    \n",
    "    def __init_weights(self):\n",
    "        '''Initializes weights to a dummy value which is better for visualization'''\n",
    "        self.weights_ = []\n",
    "        self.weights_.append(np.array([[-10.0,-10.0]]).T)\n",
    "    \n",
    "    def fit(self, Xs, Ys, epochs, lr=1e-3, gamma=0.9):\n",
    "        '''Trains the model on the given dataset for \"epoch\" number of itterations with step size=\"lr\". \n",
    "        Returns list containing loss for each epoch as well as lists containing weight values for each epoch.'''\n",
    "        history = []\n",
    "        w_hist1 = []\n",
    "        w_hist2 = []\n",
    "        for epoch in tqdm_notebook(range(epochs)):\n",
    "            activations = self.forward_pass(Xs)\n",
    "            deltas = self.backward_pass(Ys, activations)\n",
    "            \n",
    "            layer_inputs = [Xs] + activations[:-1]\n",
    "            self.weight_update(deltas, layer_inputs, lr, gamma)\n",
    "                    \n",
    "            preds = self.predict(Xs)\n",
    "            current_loss = self.mean_squared_error(preds, Ys)\n",
    "            \n",
    "            history.append(current_loss)\n",
    "            w_hist1.append(self.weights_[0][0][0])\n",
    "            w_hist2.append(self.weights_[0][1][0])\n",
    "        return history, w_hist1, w_hist2\n",
    "    \n",
    "    def forward_pass(self, input_data):\n",
    "        '''Executes the feed forward algorithm.\n",
    "        \"input_data\" is the input to the network in row-major form\n",
    "        Returns \"activations\", which is a list of all layer outputs (excluding input layer of course)'''\n",
    "        activations = []\n",
    "        \n",
    "        a = input_data.dot(self.weights_[0])\n",
    "        z = self.sigmoid(a)\n",
    "        activations.append(z)\n",
    "        \n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, targets, layer_activations):\n",
    "        '''Executes the backpropogation algorithm.\n",
    "        \"targets\" is the ground truth/labels\n",
    "        \"layer_activations\" are the return value of the forward pass step\n",
    "        Returns \"deltas\", which is a list containing weight update values for all layers (excluding the input layer of course)'''\n",
    "        deltas = []\n",
    "        \n",
    "        activation_deriv = np.multiply(layer_activations[-1], 1-layer_activations[-1])\n",
    "        delta = np.multiply((layer_activations[-1] - targets), activation_deriv)\n",
    "        deltas.append(delta)\n",
    "        \n",
    "        return deltas\n",
    "    \n",
    "    def weight_update(self, deltas, layer_inputs, lr, gamma):\n",
    "        '''Executes the gradient descent algorithm.\n",
    "        \"deltas\" is return value of the backward pass step\n",
    "        \"layer_inputs\" is a list containing the inputs for all layers (including the input layer)\n",
    "        \"lr\" is the learning rate\n",
    "        \"gamma\" is the momentum parameter'''\n",
    "        gradient = np.dot(deltas[0].T, layer_inputs[0]).T\n",
    "\n",
    "        if self.optimizer == 'sgd':\n",
    "            update_amount = lr * gradient\n",
    "        elif self.optimizer == 'momentum':\n",
    "            raise NotImplementedError(f'The {self.optimizer} optimizer has not been implemented yet.')\n",
    "        elif self.optimizer == 'nag':\n",
    "            raise NotImplementedError(f'The {self.optimizer} optimizer has not been implemented yet.')\n",
    "        elif self.optimizer == 'rmsprop':\n",
    "            raise NotImplementedError(f'The {self.optimizer} optimizer has not been implemented yet.')\n",
    "        elif self.optimizer == 'adam':\n",
    "            raise NotImplementedError(f'The {self.optimizer} optimizer has not been implemented yet.')\n",
    "        elif self.optimizer == 'adagrad':\n",
    "            raise NotImplementedError(f'The {self.optimizer} optimizer has not been implemented yet.')\n",
    "\n",
    "        self.weights_[0] -= update_amount\n",
    "        \n",
    "    def predict(self, Xs):\n",
    "        '''Returns the model predictions (output of the last layer) for the given \"Xs\".'''\n",
    "        return self.forward_pass(Xs)[-1]\n",
    "    \n",
    "    def evaluate(self, Xs, Ys):\n",
    "        '''Returns total MSE for given dataset'''\n",
    "        pred = self.predict(Xs)\n",
    "        return self.mean_squared_error(pred, Ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWF7Xn1er1fx"
   },
   "source": [
    "__This is a simple regression dataset, you can put any arbitrary binary function here and see how the error surface changes.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T10:49:03.390022Z",
     "start_time": "2019-02-12T10:49:03.314559Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XdBGz_l6r1fy"
   },
   "outputs": [],
   "source": [
    "a = np.random.uniform(low=0.0, high=0.5, size=(150,))\n",
    "b = np.random.uniform(low=0.0, high=0.5, size=(150,))\n",
    "dataset = pd.DataFrame({\n",
    "    'var1':   a,\n",
    "    'var2':   b,\n",
    "    # 'output': a+b,\n",
    "    'output': (a**b)+b,\n",
    "})\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AEJJ1bsIr1f1"
   },
   "source": [
    "__We train neural networks for all the optimizers and obtain the histories of parameter updates for them. \n",
    "These (except GD) will not work for you untill you implement them yourself.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T11:08:05.748117Z",
     "start_time": "2019-02-12T11:08:05.742110Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jJNFA0ZPr1f2"
   },
   "outputs": [],
   "source": [
    "# write name(nag, rmsprop, adam, adagrad) of two optional optimizers that you have implemented \n",
    "opt1 = 'nag'\n",
    "opt2 = 'rmsprop'\n",
    "\n",
    "epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T10:53:44.489378Z",
     "start_time": "2019-02-12T10:53:43.960129Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "x-a0N7avr1f6"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork('sgd')\n",
    "_,w1h_sgd,w2h_sgd = nn.fit(dataset[['var1','var2']].values, dataset[['output']].values, \n",
    "                                 epochs=epochs, lr=0.01, gamma=0.9)\n",
    "\n",
    "nn = NeuralNetwork('momentum')\n",
    "_,w1h_mome,w2h_mome = nn.fit(dataset[['var1','var2']].values, dataset[['output']].values, \n",
    "                                   epochs=epochs, lr=0.01, gamma=0.9)\n",
    "\n",
    "\n",
    "nn = NeuralNetwork(opt1)\n",
    "_,w1h_opt1,w2h_opt1 = nn.fit(dataset[['var1','var2']].values, dataset[['output']].values, \n",
    "                                   epochs=epochs, lr=0.01, gamma=0.9)\n",
    "\n",
    "nn = NeuralNetwork(opt2)\n",
    "_,w1h_opt2,w2h_opt2 = nn.fit(dataset[['var1','var2']].values, dataset[['output']].values, \n",
    "                                   epochs=epochs, lr=0.01, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjjWumhTr1f-"
   },
   "source": [
    "__You are not required to understand the inner workings of this plotting function, however you should be able to changes ranges of x and y to control how much of the error surface is shown.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "MCzvYVCRr1f_"
   },
   "outputs": [],
   "source": [
    "def make_viz(t):\n",
    "    x = np.arange(-30,30,0.5)\n",
    "    y = np.arange(-20,20,0.5)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    z = get_error_surface(x, y, \n",
    "                          NeuralNetwork, \n",
    "                          (dataset[['var1','var2']].values, dataset[['output']].values))\n",
    "    if len(plt.gcf().axes) == 4: \n",
    "        CS = ax1.contourf(x, y, z, levels=10, cmap='Purples');\n",
    "        ax1.clabel(CS, inline=False, fontsize=10, colors='k')\n",
    "\n",
    "        CS = ax2.contourf(x, y, z, levels=10, cmap='Purples');\n",
    "        ax2.clabel(CS, inline=False, fontsize=10, colors='k')\n",
    "    \n",
    "        CS = ax3.contourf(x, y, z, levels=10, cmap='Purples');\n",
    "        ax3.clabel(CS, inline=False, fontsize=10, colors='k')\n",
    "    \n",
    "        CS = ax4.contourf(x, y, z, levels=10, cmap='Purples');\n",
    "        ax4.clabel(CS, inline=False, fontsize=10, colors='k')\n",
    "\n",
    "    ax1.plot(w1h_sgd[:t], w2h_sgd[:t], \n",
    "             marker='.', markersize=4, c='tab:green');\n",
    "    ax1.set_title(f'SGD - Epoch {t}');\n",
    "\n",
    "    ax2.plot(w1h_mome[:t], w2h_mome[:t], \n",
    "             marker='.', markersize=4, c='tab:red');\n",
    "    ax2.set_title(f'Momentum - Epoch {t}');\n",
    "    \n",
    "    ax3.plot(w1h_opt1[:t], w2h_opt1[:t], \n",
    "             marker='.', markersize=4, c='tab:red');\n",
    "    ax3.set_title(f'{opt1} - Epoch {t}');\n",
    "    \n",
    "    ax4.plot(w1h_opt2[:t], w2h_opt2[:t], \n",
    "             marker='.', markersize=4, c='tab:red');\n",
    "    ax4.set_title(f'{opt2} - Epoch {t}');\n",
    "\n",
    "    if len(plt.gcf().axes) == 4: \n",
    "        fig.subplots_adjust(right=0.8)\n",
    "        cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "        fig.colorbar(CS, cax=cbar_ax)\n",
    "    \n",
    "fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(ncols=2, nrows=2, figsize=(15,12), dpi=80)\n",
    "fig.suptitle(f'Error Surfaces {rollnumber}')\n",
    "play = Play(min=0,max=epochs,step=10,value=0)\n",
    "interact(make_viz, t=play);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "AbGdieoNr1gC"
   },
   "source": [
    "## Task 2 - Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "X1h4q_QEr1gE"
   },
   "source": [
    "We discussed in class how, due to the complex shape of the error surface, the optimization is very sensitive to the initial parameter values. We will now see this in action by using three completely different weight initialization strategies:\n",
    "- Zero initialization\n",
    "- Random Normal Initialization ($\\mu = 0, \\sigma = 10$)\n",
    "- [He Initialization](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf) with variance $\\sqrt{\\frac{2}{size(l-1)}}$\n",
    "- [Xavier Initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) with variance ${\\frac{1}{size(l-1)}}$\n",
    "\n",
    "You will only modify the `_init_weights` method. No other method needs to be changed. The `_init_weights` method already contains code for random initialization with standard normal ($\\mu = 0, \\sigma = 1$). \n",
    "\n",
    "We will be using simple zero initialization for biases.\n",
    "\n",
    "Vizualization code is provided to plot the decision boundary of the binary classification function your model will learn. The different initialization methods will yeild different decision boundaries (some might not converge at all). This effect can also be seen in the plot of error as a function of time/epochs.\n",
    "\n",
    "Since we are using a very simple dataset, the difference between random normal and normalized initializations will not be that pronounced in the decision boundary, however the training plot might reveal more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T11:20:04.238627Z",
     "start_time": "2019-02-12T11:20:04.225841Z"
    },
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "mhG_Ogtwr1gF"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T13:31:54.404608Z",
     "start_time": "2019-02-12T13:31:54.327757Z"
    },
    "code_folding": [
     0,
     103,
     112,
     121
    ],
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "weG3wdRIr1gJ"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    @staticmethod\n",
    "    def cross_entropy_loss(y_pred, y_true):\n",
    "        return -(y_true * np.log(y_pred)).sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(y_pred, y_true):\n",
    "        return np.sum(y_pred == y_true)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        expx = np.exp(x)\n",
    "        return expx / expx.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def __init__(self, nodes_per_layer, init_strategy='standard_normal'):\n",
    "        '''Creates a Feed-Forward Neural Network.\n",
    "        \"nodes_per_layer\" is a list containing number of nodes in each layer (including input layer)\n",
    "        \"init_strategy\" is the weight initialization formula used\n",
    "        \"mode\" can be one of 'regression' or 'classification' and controls the output activation as well as training metric'''\n",
    "        if len(nodes_per_layer) < 2:\n",
    "            raise ValueError('Network must have atleast 2 layers (input and output).')\n",
    "        if not (np.array(nodes_per_layer) > 0).all():\n",
    "            raise ValueError('Number of nodes in all layers must be positive.')\n",
    "        \n",
    "        self.num_layers = len(nodes_per_layer) # includes input layer\n",
    "        self.nodes_per_layer = nodes_per_layer\n",
    "        self.input_shape = nodes_per_layer[0]\n",
    "        self.output_shape = nodes_per_layer[-1]\n",
    "        self.init_strategy = init_strategy\n",
    "        \n",
    "        self.__init_weights(nodes_per_layer)\n",
    "    \n",
    "    def __init_weights(self, nodes_per_layer):\n",
    "        '''Initializes all weights based on standard normal distribution and all biases to 0.'''\n",
    "        self.weights_ = []\n",
    "        self.biases_ = []\n",
    "        for i,_ in enumerate(nodes_per_layer):\n",
    "            if i == 0:\n",
    "                # skip input layer, it does not have weights/bias\n",
    "                continue\n",
    "            \n",
    "            if self.init_strategy == 'standard_normal':\n",
    "            \tweight_matrix = np.random.normal(size=(nodes_per_layer[i-1], nodes_per_layer[i]))\n",
    "            elif self.init_strategy == 'zeros':\n",
    "                raise NotImplementedError(f'The {self.init_strategy} strategy has not been implemented yet.')\n",
    "            elif self.init_strategy == 'normal':\n",
    "                raise NotImplementedError(f'The {self.init_strategy} strategy has not been implemented yet.')\n",
    "            elif self.init_strategy == 'he':\n",
    "                raise NotImplementedError(f'The {self.init_strategy} strategy has not been implemented yet.')\n",
    "            elif self.init_strategy == 'xavior':\n",
    "                raise NotImplementedError(f'The {self.init_strategy} strategy has not been implemented yet.')\n",
    "\n",
    "            self.weights_.append(weight_matrix)\n",
    "            bias_vector = np.zeros(shape=(nodes_per_layer[i],))\n",
    "            self.biases_.append(bias_vector)\n",
    "    \n",
    "    def fit(self, Xs, Ys, epochs, lr=1e-3):\n",
    "        '''Trains the model on the given dataset for \"epoch\" number of itterations with step size=\"lr\". \n",
    "        Returns list containing loss for each epoch.'''\n",
    "        history = []\n",
    "        for epoch in tqdm_notebook(range(epochs)):\n",
    "            activations = self.forward_pass(Xs)\n",
    "            deltas = self.backward_pass(Ys, activations)\n",
    "            \n",
    "            layer_inputs = [Xs] + activations[:-1]\n",
    "            self.weight_update(deltas, layer_inputs, lr)\n",
    "                    \n",
    "            preds = self.predict(Xs)\n",
    "            current_loss = self.cross_entropy_loss(preds, Ys)\n",
    "            history.append(current_loss)\n",
    "        return history\n",
    "    \n",
    "    def forward_pass(self, input_data):\n",
    "        '''Executes the feed forward algorithm.\n",
    "        \"input_data\" is the input to the network in row-major form\n",
    "        Returns \"activations\", which is a list of all layer outputs (excluding input layer of course)'''\n",
    "        activations = []\n",
    "        next_layer_input = input_data\n",
    "        for i in range(self.num_layers-1):\n",
    "            intermediate = next_layer_input.dot(self.weights_[i]) + self.biases_[i]\n",
    "            if i == self.num_layers-2: # output layer\n",
    "                next_layer_input = self.softmax(intermediate)\n",
    "            else:\n",
    "                next_layer_input = self.sigmoid(intermediate)\n",
    "            activations.append(next_layer_input)\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, targets, layer_activations):\n",
    "        '''Executes the backpropogation algorithm.\n",
    "        \"targets\" is the ground truth/labels\n",
    "        \"layer_activations\" are the return value of the forward pass step\n",
    "        Returns \"deltas\", which is a list containing weight update values for all layers (excluding the input layer of course)'''\n",
    "        deltas = [0] * self.num_layers\n",
    "        deltas[-1] = layer_activations[-1] - targets\n",
    "        \n",
    "        for i in reversed(range(1, self.num_layers-1)): # skip input and output layers\n",
    "            err = np.dot(self.weights_[i], deltas[i+1].T).T\n",
    "            activation_deriv = np.multiply(layer_activations[i-1], 1-layer_activations[i-1])\n",
    "            deltas[i] = np.multiply(err, activation_deriv)\n",
    "        return deltas[1:]\n",
    "\n",
    "    def weight_update(self, deltas, layer_inputs, lr):\n",
    "        '''Executes the gradient descent algorithm.\n",
    "        \"deltas\" is return value of the backward pass step\n",
    "        \"layer_inputs\" is a list containing the inputs for all layers (including the input layer)\n",
    "        \"lr\" is the learning rate'''\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.weights_[i] -= lr * np.dot(deltas[i].T, layer_inputs[i]).T\n",
    "            self.biases_[i]  -= lr * deltas[i].sum(axis=0)\n",
    "        \n",
    "    def predict(self, Xs):\n",
    "        '''Returns the model predictions (output of the last layer) for the given \"Xs\".'''\n",
    "        return self.forward_pass(Xs)[-1]\n",
    "    \n",
    "    def evaluate(self, Xs, Ys):\n",
    "        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''\n",
    "        pred = self.predict(Xs)\n",
    "        return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))\n",
    "    \n",
    "    def plot_model(self, filename):\n",
    "        '''Provide the \"filename\" as a string including file extension. Creates an image showing the model as a graph.'''\n",
    "        graph = pydot.Dot(graph_type='digraph')\n",
    "        graph.set_rankdir('LR')\n",
    "        graph.set_node_defaults(shape='circle', fontsize=0)\n",
    "        for i in range(self.num_layers-1):\n",
    "            for n1 in range(self.nodes_per_layer[i]):\n",
    "                for n2 in range(self.nodes_per_layer[i+1]):\n",
    "                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')\n",
    "                    graph.add_edge(edge)\n",
    "        graph.write_png(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T11:22:13.702789Z",
     "start_time": "2019-02-12T11:22:13.161591Z"
    },
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "eEEim8pMr1gM"
   },
   "outputs": [],
   "source": [
    "data_x, data_y = make_circles(500, noise=0.07)\n",
    "plt.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);\n",
    "plt.gca().set(xlabel='Feature 1', ylabel='Feature 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "RMkMg_Nzr1gQ"
   },
   "source": [
    "### Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "4KpOfI1mr1gS"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2,10,10,2], init_strategy='zeros')\n",
    "history = nn.fit(data_x, pd.get_dummies(data_y).values, epochs=5000, lr=1e-2)\n",
    "plt.plot(history);\n",
    "plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title=f'Training Plot {rollnumber}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "YNAG3kDfr1gW"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = data_x[:, 0].min() - .5, data_x[:, 0].max() + .5\n",
    "y_min, y_max = data_x[:, 1].min() - .5, data_x[:, 1].max() + .5\n",
    "plot_decision_boundary(lambda x: nn.predict(x).argmax(axis=1), \n",
    "                       x_min, x_max, y_min, y_max, \n",
    "                       plt.cm.Spectral, ax)\n",
    "ax.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2', title=f'Neural Network Classifier {rollnumber}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "EOIKUI6Wr1gZ"
   },
   "source": [
    "### Random Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "QhOsqLarr1gZ"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2,10,10,2], init_strategy='normal')\n",
    "history = nn.fit(data_x, pd.get_dummies(data_y).values, epochs=5000, lr=1e-2)\n",
    "plt.plot(history);\n",
    "plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title=f'Training Plot {rollnumber}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "_MnFSQMNr1gc"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = data_x[:, 0].min() - .5, data_x[:, 0].max() + .5\n",
    "y_min, y_max = data_x[:, 1].min() - .5, data_x[:, 1].max() + .5\n",
    "plot_decision_boundary(lambda x: nn.predict(x).argmax(axis=1), \n",
    "                       x_min, x_max, y_min, y_max, \n",
    "                       plt.cm.Spectral, ax)\n",
    "ax.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2', title=f'Neural Network Classifier {rollnumber}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "BZ2SmLkor1gf"
   },
   "source": [
    "### He"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "aua5zTmLr1gj"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2,10,10,2], init_strategy='he')\n",
    "history = nn.fit(data_x, pd.get_dummies(data_y).values, epochs=5000, lr=1e-2)\n",
    "plt.plot(history);\n",
    "plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title=f'Training Plot {rollnumber}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "JcoZyRCar1gm"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = data_x[:, 0].min() - .5, data_x[:, 0].max() + .5\n",
    "y_min, y_max = data_x[:, 1].min() - .5, data_x[:, 1].max() + .5\n",
    "plot_decision_boundary(lambda x: nn.predict(x).argmax(axis=1), \n",
    "                       x_min, x_max, y_min, y_max, \n",
    "                       plt.cm.Spectral, ax)\n",
    "ax.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2', title=f'Neural Network Classifier {rollnumber}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "cI5Qpa3QyAzM"
   },
   "source": [
    "### Xavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "5JGPwoYxyAzQ"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2,10,10,2], init_strategy='xavior')\n",
    "history = nn.fit(data_x, pd.get_dummies(data_y).values, epochs=5000, lr=1e-2)\n",
    "plt.plot(history);\n",
    "plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title=f'Training Plot {rollnumber}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "DciMkIH3yAzV"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = data_x[:, 0].min() - .5, data_x[:, 0].max() + .5\n",
    "y_min, y_max = data_x[:, 1].min() - .5, data_x[:, 1].max() + .5\n",
    "plot_decision_boundary(lambda x: nn.predict(x).argmax(axis=1), \n",
    "                       x_min, x_max, y_min, y_max, \n",
    "                       plt.cm.Spectral, ax)\n",
    "ax.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2', title=f'Neural Network Classifier {rollnumber}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "IDU7HArQr1gp"
   },
   "source": [
    "## Task 3 - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "SSFDxHGhr1gq"
   },
   "source": [
    "Now we will explore the propensity of Neural Networks to overfit the training data. We will be identifying this phenomenon visually by inspecting the decision boundary, rather than evaluating the model on a test/holdout dataset. \n",
    "\n",
    "You will implement __both__ $L_1$ and $L_2$ regularizers in this task.\n",
    "\n",
    "The only function you need to modify is `weight_update` (or `backward_pass`), `__l1__` and `__l2__` since the `__init__` and `fit` functions have been modified for you.\n",
    "\n",
    "*Note:* You can add the derivative of the regularizing term in either of the 2 functions dealing with derivatives (it would be simpler in `weight_update`).\n",
    "\n",
    "Vizualization code is provided to plot the decision boundary of the binary classification function your model will learn. The different regularizers (including no regularization) will show up as different fits over the training data in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T14:30:00.361624Z",
     "start_time": "2019-02-12T14:30:00.254751Z"
    },
    "code_folding": [
     0,
     42,
     85,
     100,
     128,
     132,
     137
    ],
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "QMufx5yUr1gr"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    @staticmethod\n",
    "    def mean_squared_error(y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy_loss(y_pred, y_true):\n",
    "        return -(y_true * np.log(y_pred)).sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(y_pred, y_true):\n",
    "        return np.sum(y_pred == y_true)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        expx = np.exp(x)\n",
    "        return expx / expx.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def __init__(self, nodes_per_layer, regularization='none', reg_factor=0.0):\n",
    "        '''Creates a Feed-Forward Neural Network.\n",
    "        \"nodes_per_layer\" is a list containing number of nodes in each layer (including input layer)\n",
    "        \"regularization\" is one of {l1,l2,None}\n",
    "        \"reg_factor\" is teh lambda parameter in the regularized loss equation \n",
    "        \"mode\" can be one of 'regression' or 'classification' and controls the output activation as well as training metric'''\n",
    "        if len(nodes_per_layer) < 2:\n",
    "            raise ValueError('Network must have atleast 2 layers (input and output).')\n",
    "        if not (np.array(nodes_per_layer) > 0).all():\n",
    "            raise ValueError('Number of nodes in all layers must be positive.')\n",
    "        \n",
    "        self.num_layers = len(nodes_per_layer) # includes input layer\n",
    "        self.nodes_per_layer = nodes_per_layer\n",
    "        self.input_shape = nodes_per_layer[0]\n",
    "        self.output_shape = nodes_per_layer[-1]\n",
    "        self.regularization = regularization\n",
    "        self.reg_factor = reg_factor\n",
    "        \n",
    "        self.__init_weights(nodes_per_layer)\n",
    "    \n",
    "    def __init_weights(self, nodes_per_layer):\n",
    "        '''Initializes all weights based on standard normal distribution and all biases to 0.'''\n",
    "        self.weights_ = []\n",
    "        self.biases_ = []\n",
    "        for i,_ in enumerate(nodes_per_layer):\n",
    "            if i == 0:\n",
    "                # skip input layer, it does not have weights/bias\n",
    "                continue\n",
    "            \n",
    "            weight_matrix = np.random.normal(size=(nodes_per_layer[i-1], nodes_per_layer[i]))\n",
    "            self.weights_.append(weight_matrix)\n",
    "            bias_vector = np.zeros(shape=(nodes_per_layer[i],))\n",
    "            self.biases_.append(bias_vector)\n",
    "    \n",
    "    def __l1__(self):\n",
    "        pass\n",
    "    \n",
    "    def __l2__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, Xs, Ys, epochs, lr=1e-3):\n",
    "        '''Trains the model on the given dataset for \"epoch\" number of itterations with step size=\"lr\". \n",
    "        Returns list containing loss for each epoch.'''\n",
    "        history = []\n",
    "        for epoch in tqdm_notebook(range(epochs)):\n",
    "            activations = self.forward_pass(Xs)\n",
    "            deltas = self.backward_pass(Ys, activations)\n",
    "            \n",
    "            layer_inputs = [Xs] + activations[:-1]\n",
    "            self.weight_update(deltas, layer_inputs, lr)\n",
    "                    \n",
    "            preds = self.predict(Xs)\n",
    "            \n",
    "            regularization_amount = 0\n",
    "            if self.regularization == 'l1':\n",
    "                regularization_amount = self.__l1__()\n",
    "            elif self.regularization == 'l2':\n",
    "                regularization_amount = self.__l2__()\n",
    "            \n",
    "            current_loss = self.cross_entropy_loss(preds, Ys) + regularization_amount\n",
    "            history.append(current_loss)\n",
    "        return history\n",
    "    \n",
    "    def forward_pass(self, input_data):\n",
    "        '''Executes the feed forward algorithm.\n",
    "        \"input_data\" is the input to the network in row-major form\n",
    "        Returns \"activations\", which is a list of all layer outputs (excluding input layer of course)'''\n",
    "        activations = []\n",
    "        next_layer_input = input_data\n",
    "        for i in range(self.num_layers-1):\n",
    "            intermediate = next_layer_input.dot(self.weights_[i]) + self.biases_[i]\n",
    "            if i == self.num_layers-2: # output layer\n",
    "                next_layer_input = self.softmax(intermediate)\n",
    "            else:\n",
    "                next_layer_input = self.sigmoid(intermediate)\n",
    "            activations.append(next_layer_input)\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, targets, layer_activations):\n",
    "        '''Executes the backpropogation algorithm.\n",
    "        \"targets\" is the ground truth/labels\n",
    "        \"layer_activations\" are the return value of the forward pass step\n",
    "        Returns \"deltas\", which is a list containing weight update values for all layers (excluding the input layer of course)'''\n",
    "        deltas = [0] * self.num_layers\n",
    "        deltas[-1] = layer_activations[-1] - targets\n",
    "        \n",
    "        for i in reversed(range(1, self.num_layers-1)): # skip input and output layers\n",
    "            err = np.dot(self.weights_[i], deltas[i+1].T).T\n",
    "            activation_deriv = np.multiply(layer_activations[i-1], 1-layer_activations[i-1])\n",
    "            deltas[i] = np.multiply(err, activation_deriv)\n",
    "        return deltas[1:]\n",
    "\n",
    "    def weight_update(self, deltas, layer_inputs, lr):\n",
    "        '''Executes the gradient descent algorithm.\n",
    "        \"deltas\" is return value of the backward pass step\n",
    "        \"layer_inputs\" is a list containing the inputs for all layers (including the input layer)\n",
    "        \"lr\" is the learning rate'''\n",
    "        for i in range(self.num_layers-1):\n",
    "            if self.regularization == 'l1':\n",
    "                raise NotImplementedError(f'{self.regularization} regularization is not implemented.')\n",
    "            elif self.regularization == 'l2':\n",
    "                raise NotImplementedError(f'{self.regularization} regularization is not implemented.')\n",
    "            elif self.regularization == 'none':\n",
    "                self.weights_[i] -= lr * np.dot(deltas[i].T, layer_inputs[i]).T\n",
    "            self.biases_[i]  -= lr * deltas[i].sum(axis=0)\n",
    "        \n",
    "    def predict(self, Xs):\n",
    "        '''Returns the model predictions (output of the last layer) for the given \"Xs\".'''\n",
    "        return self.forward_pass(Xs)[-1]\n",
    "    \n",
    "    def evaluate(self, Xs, Ys):\n",
    "        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''\n",
    "        pred = self.predict(Xs)\n",
    "        return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))\n",
    "    \n",
    "    def plot_model(self, filename):\n",
    "        '''Provide the \"filename\" as a string including file extension. Creates an image showing the model as a graph.'''\n",
    "        graph = pydot.Dot(graph_type='digraph')\n",
    "        graph.set_rankdir('LR')\n",
    "        graph.set_node_defaults(shape='circle', fontsize=0)\n",
    "        for i in range(self.num_layers-1):\n",
    "            for n1 in range(self.nodes_per_layer[i]):\n",
    "                for n2 in range(self.nodes_per_layer[i+1]):\n",
    "                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')\n",
    "                    graph.add_edge(edge)\n",
    "        graph.write_png(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "ggCDlc7jr1gu"
   },
   "outputs": [],
   "source": [
    "data_x, data_y = make_blobs(500, centers=2, cluster_std=6)\n",
    "plt.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);\n",
    "plt.gca().set(xlabel='Feature 1', ylabel='Feature 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "ZnqI_ftir1gx"
   },
   "source": [
    "### No Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "_JtuJtpSr1gy"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2,10,10,2], regularization='none', reg_factor=0.0)\n",
    "history = nn.fit(data_x, pd.get_dummies(data_y).values, epochs=5000, lr=1e-2)\n",
    "plt.plot(history);\n",
    "plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title=f'Training Plot {rollnumber}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "rzTU87PFr1g1"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = data_x[:, 0].min() - .5, data_x[:, 0].max() + .5\n",
    "y_min, y_max = data_x[:, 1].min() - .5, data_x[:, 1].max() + .5\n",
    "plot_decision_boundary(lambda x: nn.predict(x).argmax(axis=1), \n",
    "                       x_min, x_max, y_min, y_max, \n",
    "                       plt.cm.Spectral, ax)\n",
    "ax.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2', title=f'Neural Network Classifier {rollnumber}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "T8sOj9Ndr1g5"
   },
   "source": [
    "### $L_1$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "XL8dPh0br1g6"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2,10,10,2], regularization='l1', reg_factor=0.3)\n",
    "history = nn.fit(data_x, pd.get_dummies(data_y).values, epochs=5000, lr=1e-2)\n",
    "plt.plot(history);\n",
    "plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title=f'Training Plot {rollnumber}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "-HRYDTuFr1g9"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = data_x[:, 0].min() - .5, data_x[:, 0].max() + .5\n",
    "y_min, y_max = data_x[:, 1].min() - .5, data_x[:, 1].max() + .5\n",
    "plot_decision_boundary(lambda x: nn.predict(x).argmax(axis=1), \n",
    "                       x_min, x_max, y_min, y_max, \n",
    "                       plt.cm.Spectral, ax)\n",
    "ax.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2', title=f'Neural Network Classifier {rollnumber}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "ZEAgoCVyr1hA"
   },
   "source": [
    "### $L_2$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "yEPfS2Zpr1hC"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2,10,10,2], regularization='l2', reg_factor=0.3)\n",
    "history = nn.fit(data_x, pd.get_dummies(data_y).values, epochs=5000, lr=1e-2)\n",
    "plt.plot(history);\n",
    "plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title=f'Training Plot {rollnumber}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "WInV_rL5r1hG"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = data_x[:, 0].min() - .5, data_x[:, 0].max() + .5\n",
    "y_min, y_max = data_x[:, 1].min() - .5, data_x[:, 1].max() + .5\n",
    "plot_decision_boundary(lambda x: nn.predict(x).argmax(axis=1), \n",
    "                       x_min, x_max, y_min, y_max, \n",
    "                       plt.cm.Spectral, ax)\n",
    "ax.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2', title=f'Neural Network Classifier {rollnumber}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "HKfVVlO4r1hK"
   },
   "source": [
    "## Task 4 - Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "BnYFu8rQr1hM"
   },
   "source": [
    "Now we will see how the choice of error function determines the performance of the model for a particular task. This task will also serve as a practical introduction to the basics of Keras.\n",
    "\n",
    "The code to create and evaluate the model has been provided to you already. All you need to do is fill in the `mean_squared_error`, `cross_entropy_loss` and `root_mean_squared_logarithmic_error` functions and use them appropriately. \n",
    "\n",
    "You will use all these loss functions to learn regression as well as classification and then compare the results via the plots.\n",
    "\n",
    "You should keep an eye on the loss plot generated after training and the values of evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "-atgfImDr1hO"
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T07:02:35.943246Z",
     "start_time": "2019-02-12T07:02:35.926955Z"
    },
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "nE8YGrIgr1hO"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 1\n",
    "class_labels = np.arange(0, num_classes, 1)\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T07:33:36.497335Z",
     "start_time": "2019-02-12T07:33:36.487451Z"
    },
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "6wuOJT0Tr1hS"
   },
   "outputs": [],
   "source": [
    "data_x, data_y = make_moons(200, noise=0.18)\n",
    "plt.scatter(data_x[:,0], data_x[:,1]);\n",
    "plt.gca().set(xlabel='Input', ylabel='Output');\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.30, shuffle=True)\n",
    "x_train = data_x[:, 0]\n",
    "y_train = data_y\n",
    "x_test, y_test = make_moons(100, noise=0.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "hidden": true,
    "id": "ElyzZ9G2r1hW",
    "outputId": "249a27c4-da4a-4e07-8862-8bcfa2c61bfa"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(1,), name='input_l'))\n",
    "model.add(Dense(50, activation='relu', name='hidden_l'))\n",
    "model.add(Dense(num_classes, activation='relu', name='output_l'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T07:34:53.337575Z",
     "start_time": "2019-02-12T07:34:53.317596Z"
    },
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "NHV0ZQe0r1hY"
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    pass\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    pass\n",
    "\n",
    "def root_mean_squared_logarithmic_error(y_pred, y_true):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "oWzKidoGr1hd"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=mean_squared_error,\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "stbxd9Alr1hg"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "hidden": true,
    "id": "pyqKmoNor1hk",
    "outputId": "a90dd3cb-5890-4f81-d6bf-57931d2eaa3b"
   },
   "outputs": [],
   "source": [
    "h = pd.DataFrame(history.history)\n",
    "\n",
    "h[['loss']].plot();\n",
    "plt.gca().set_xlabel('Epoch');\n",
    "plt.gca().set_ylabel('MSE');\n",
    "plt.gca().set_title(f'{rollnumber}');\n",
    "\n",
    "h[['accuracy']].plot();\n",
    "plt.gca().set_xlabel('Epoch');\n",
    "plt.gca().set_ylabel('Accuracy');\n",
    "plt.gca().set_title(f'{rollnumber}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "uiutgB1Ur1hn"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test[:,0], y_test, \n",
    "                       verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "OQuuwH7Ar1hq"
   },
   "outputs": [],
   "source": [
    "d = np.arange(-1.5, 2.5, 0.1).reshape((40,1))\n",
    "preds = model.predict(d, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "vA_b9Ydwr1hu"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));\n",
    "ax.plot(d.flatten(), preds.flatten(), c='tab:red', label='Prediction');\n",
    "ax.set(xlabel='Input', ylabel='Output', title=f'Neural Network Regression {rollnumber}');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "1Z7c2dpdr1hz"
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "TVV1jvySr1h4"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 2\n",
    "class_labels = np.arange(0,num_classes,1)\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T07:33:36.497335Z",
     "start_time": "2019-02-12T07:33:36.487451Z"
    },
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "jvGk6Gwhr1h7"
   },
   "outputs": [],
   "source": [
    "data_x, data_y = make_moons(200, noise=0.20)\n",
    "plt.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);\n",
    "plt.gca().set(xlabel='Feature 1', ylabel='Feature 2');\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.30, shuffle=True)\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "hidden": true,
    "id": "RfuKLFgPr1h9",
    "outputId": "e3674834-c821-4ec3-8703-b4f3872d95e2"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2,), name='input_l'))\n",
    "model.add(Dense(50, activation='relu', name='hidden_l'))\n",
    "model.add(Dense(num_classes, activation='softmax', name='output_l'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T07:34:53.337575Z",
     "start_time": "2019-02-12T07:34:53.317596Z"
    },
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "_fwItp0Gr1iD"
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_pred, y_true):\n",
    "    pass\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    pass\n",
    "\n",
    "def root_mean_squared_logarithmic_error(y_pred, y_true):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "zODemwEjr1iF"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=mean_squared_error,\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "yJYWrp8xr1iJ"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "dsV-lGxKr1iM"
   },
   "outputs": [],
   "source": [
    "h = pd.DataFrame(history.history)\n",
    "\n",
    "h[['loss']].plot();\n",
    "plt.gca().set_xlabel('Epoch');\n",
    "plt.gca().set_ylabel('MSE');\n",
    "plt.gca().set_title(f'{rollnumber}');\n",
    "\n",
    "h[['accuracy']].plot();\n",
    "plt.gca().set_xlabel('Epoch');\n",
    "plt.gca().set_ylabel('Accuracy');\n",
    "plt.gca().set_title(f'{rollnumber}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "BgfAs6Dtr1iO"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, \n",
    "                       verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "K6k9_tHvr1iQ"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = data_x[:, 0].min() - .5, data_x[:, 0].max() + .5\n",
    "y_min, y_max = data_x[:, 1].min() - .5, data_x[:, 1].max() + .5\n",
    "plot_decision_boundary(lambda x: model.predict(x).argmax(axis=1), \n",
    "                       x_min, x_max, y_min, y_max, \n",
    "                       plt.cm.Spectral, ax)\n",
    "ax.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2', title=f'Neural Network Classifier {rollnumber}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RMkMg_Nzr1gQ",
    "EOIKUI6Wr1gZ",
    "BZ2SmLkor1gf",
    "cI5Qpa3QyAzM",
    "IDU7HArQr1gp",
    "ZnqI_ftir1gx",
    "T8sOj9Ndr1g5",
    "ZEAgoCVyr1hA",
    "1Z7c2dpdr1hz"
   ],
   "name": "CS437_DL_HW2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
